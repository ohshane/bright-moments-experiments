{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "__file__ = Path(os.path.realpath(\"__file__\"))\n",
    "project_path = __file__.parent\n",
    "dataset_path = Path.home() / \"Dataset\" / \"CelebA\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from collections import namedtuple\n",
    "import csv\n",
    "import PIL\n",
    "from typing import Any, Callable, List, Optional, Union, Tuple\n",
    "\n",
    "CSV = namedtuple(\"CSV\", [\"header\", \"index\", \"data\"])\n",
    "\n",
    "\n",
    "class CelebA(Dataset):\n",
    "    \"\"\"`Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        split (string): One of {'train', 'valid', 'test', 'all'}.\n",
    "            Accordingly dataset is selected.\n",
    "        target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
    "            or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "\n",
    "                - ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
    "                - ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
    "                - ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
    "                - ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
    "                  righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
    "\n",
    "            Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
    "\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "\n",
    "    base_folder = \"celeba\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_path,\n",
    "            split = \"train\",\n",
    "            target_type = [\"attr\"],\n",
    "            transform = None,\n",
    "            target_transform = None,\n",
    "    ) -> None:\n",
    "        self.split = split\n",
    "        split_map = {\n",
    "            \"train\": 0,\n",
    "            \"valid\": 1,\n",
    "            \"test\": 2,\n",
    "            \"all\": None,\n",
    "        }\n",
    "        \n",
    "        self.target_type = target_type\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        dataset_path = Path(dataset_path)\n",
    "        if not dataset_path.exists():\n",
    "            raise RuntimeError(\"dataset path does not exist\")\n",
    "        self.dataset_path = dataset_path\n",
    "        \n",
    "        split_ = split_map[split.lower()]\n",
    "        splits = self._load_csv(self.dataset_path / \"Eval\" / \"list_eval_partition.txt\")\n",
    "        identity = self._load_csv(self.dataset_path / \"Anno\" / \"identity_CelebA.txt\")\n",
    "        attr = self._load_csv(self.dataset_path / \"Anno\" / \"list_attr_celeba.txt\", header=1)\n",
    "        bbox = self._load_csv(self.dataset_path / \"Anno\" / \"list_bbox_celeba.txt\", header=1)\n",
    "        landmarks = self._load_csv(self.dataset_path / \"Anno\" / \"list_landmarks_celeba.txt\", header=1)\n",
    "        landmarks_align = self._load_csv(self.dataset_path / \"Anno\" / \"list_landmarks_align_celeba.txt\", header=1)\n",
    "\n",
    "        mask = slice(None) if split_ is None else (splits.data == split_).squeeze()\n",
    "\n",
    "        self.filename = splits.index\n",
    "        self.identity = identity.data[mask]\n",
    "        self.bbox = bbox.data[mask]\n",
    "        self.landmarks_align = landmarks_align.data[mask]\n",
    "        self.attr = attr.data[mask]\n",
    "        # map from {-1, 1} to {0, 1}\n",
    "        self.attr = torch.div(self.attr + 1, 2, rounding_mode='floor')\n",
    "        self.attr_names = attr.header\n",
    "\n",
    "    def _load_csv(\n",
    "        self,\n",
    "        filename,\n",
    "        header: Optional[int] = None,\n",
    "    ) -> CSV:\n",
    "        data, indices, headers = [], [], []\n",
    "\n",
    "        with open(filename) as csv_file:\n",
    "            data = list(csv.reader(csv_file, delimiter=' ', skipinitialspace=True))\n",
    "\n",
    "        if header is not None:\n",
    "            headers = data[header]\n",
    "            data = data[header + 1:]\n",
    "\n",
    "        indices = [row[0] for row in data]\n",
    "        data = [row[1:] for row in data]\n",
    "        data_int = [list(map(int, i)) for i in data]\n",
    "\n",
    "        return CSV(headers, indices, torch.tensor(data_int))\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        X = PIL.Image.open(self.dataset_path / \"img_align_celeba_png\" / str(self.filename[index].split(\".\")[0] + \".png\"))\n",
    "\n",
    "        target: Any = []\n",
    "        for t in self.target_type:\n",
    "            if t == \"attr\":\n",
    "                target.append(self.attr[index, :])\n",
    "            elif t == \"identity\":\n",
    "                target.append(self.identity[index, 0])\n",
    "            elif t == \"bbox\":\n",
    "                target.append(self.bbox[index, :])\n",
    "            elif t == \"landmarks\":\n",
    "                target.append(self.landmarks_align[index, :])\n",
    "            else:\n",
    "                # TODO: refactor with utils.verify_str_arg\n",
    "                raise ValueError(\"Target type \\\"{}\\\" is not recognized.\".format(t))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return X, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.attr)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data = CelebA(dataset_path=dataset_path, split=\"train\", target_type=[\"attr\"], transform=ToTensor())\n",
    "valid_data = CelebA(dataset_path=dataset_path, split=\"valid\", target_type=[\"attr\"], transform=ToTensor())\n",
    "test_data  = CelebA(dataset_path=dataset_path, split=\"test\",  target_type=[\"attr\"], transform=ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 218, 178])\n",
      "Labels batch shape: torch.Size([64, 40])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "input_batch, train_labels = next(iter(train_dataloader))\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "# print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/sunmoon/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([4.6437e-05, 2.7993e-04, 1.4022e-05, 4.3904e-05, 1.4072e-05, 2.1825e-05,\n",
      "        1.8124e-05, 1.4898e-05, 2.2942e-06, 1.2285e-06, 3.4518e-05, 3.7220e-06,\n",
      "        5.3221e-05, 2.1544e-05, 3.8609e-05, 5.4068e-06, 7.2105e-06, 2.4682e-05,\n",
      "        1.5075e-05, 5.7484e-05, 7.5473e-06, 2.8291e-06, 3.4276e-06, 1.2852e-05,\n",
      "        7.8343e-06, 4.2282e-06, 5.4779e-06, 1.4387e-05, 4.0712e-05, 1.1474e-04,\n",
      "        4.0393e-06, 4.4407e-06, 1.6673e-05, 1.1810e-05, 2.0024e-05, 1.6985e-05,\n",
      "        5.6110e-05, 8.9325e-06, 2.7367e-05, 4.2044e-06, 3.5585e-06, 5.9624e-06,\n",
      "        1.7508e-05, 3.3779e-05, 1.0272e-05, 4.1349e-05, 5.2873e-06, 7.9776e-06,\n",
      "        2.2164e-06, 7.4286e-06, 1.3204e-05, 6.9608e-05, 3.5796e-06, 3.4903e-06,\n",
      "        1.6521e-05, 3.5552e-06, 3.7338e-05, 3.4693e-06, 5.0624e-06, 2.0486e-05,\n",
      "        4.5229e-05, 6.2730e-05, 1.1617e-04, 2.0091e-05, 2.9312e-05, 6.8504e-06,\n",
      "        1.5239e-05, 2.1021e-05, 2.9148e-05, 7.5893e-05, 2.0104e-05, 4.9530e-04,\n",
      "        1.0213e-05, 2.2823e-05, 4.2519e-06, 3.4845e-05, 6.0735e-05, 1.1066e-05,\n",
      "        3.5695e-05, 7.1064e-05, 1.6782e-05, 2.0792e-06, 1.1441e-05, 2.6109e-05,\n",
      "        1.8148e-06, 5.4738e-06, 1.5518e-05, 7.6468e-05, 4.7602e-05, 3.5203e-06,\n",
      "        9.7885e-06, 1.1470e-05, 4.9367e-05, 4.0449e-06, 1.9488e-05, 1.3809e-05,\n",
      "        1.6293e-05, 1.0075e-06, 1.3844e-05, 4.8836e-06, 2.5186e-06, 3.7605e-06,\n",
      "        3.7913e-06, 2.8500e-05, 8.8878e-06, 2.9785e-06, 4.1557e-05, 4.0429e-04,\n",
      "        5.1740e-05, 2.7232e-05, 1.1996e-06, 1.9420e-05, 5.8029e-05, 1.8221e-05,\n",
      "        1.9932e-06, 1.5696e-06, 4.0945e-05, 4.3366e-04, 2.9941e-05, 7.4543e-06,\n",
      "        2.0473e-05, 1.8691e-05, 4.6588e-04, 8.2952e-05, 8.2784e-04, 1.9489e-05,\n",
      "        1.5087e-04, 3.8538e-06, 3.4318e-06, 1.6339e-06, 4.4470e-06, 1.2956e-06,\n",
      "        6.1758e-06, 1.7410e-06, 2.8771e-06, 2.8561e-07, 5.1185e-06, 1.7613e-06,\n",
      "        3.0376e-06, 2.6578e-06, 1.4686e-05, 1.9131e-06, 1.2197e-06, 1.3328e-06,\n",
      "        4.7234e-07, 3.1514e-05, 1.9045e-06, 1.9484e-06, 1.8163e-05, 1.0941e-05,\n",
      "        6.1132e-06, 8.1516e-05, 1.5973e-04, 8.4986e-04, 1.9310e-04, 4.3325e-04,\n",
      "        1.2819e-04, 8.4405e-05, 1.0759e-04, 3.5432e-06, 7.7392e-05, 1.9578e-05,\n",
      "        2.4740e-05, 2.7579e-05, 4.2175e-05, 2.8488e-05, 3.0804e-06, 7.6968e-06,\n",
      "        4.9604e-06, 2.4327e-06, 5.5095e-06, 5.8637e-05, 8.1901e-06, 6.4959e-06,\n",
      "        1.0802e-05, 8.9489e-06, 1.3803e-06, 9.0440e-06, 2.0816e-05, 7.4795e-05,\n",
      "        1.9223e-05, 2.3594e-05, 1.0211e-05, 2.0944e-05, 8.6478e-06, 8.4065e-05,\n",
      "        6.7872e-05, 8.4214e-04, 2.4207e-05, 1.1200e-05, 3.3350e-05, 1.2388e-05,\n",
      "        4.0653e-05, 1.9085e-04, 1.2325e-05, 6.5278e-04, 8.0923e-05, 9.9685e-06,\n",
      "        1.1027e-05, 5.8302e-05, 8.3376e-05, 1.1871e-05, 1.4095e-05, 1.6433e-04,\n",
      "        9.4715e-04, 5.2774e-06, 7.5289e-06, 9.3947e-06, 1.9475e-05, 6.7396e-06,\n",
      "        1.6300e-05, 3.6900e-05, 5.7071e-06, 9.1802e-06, 2.3192e-05, 2.2839e-05,\n",
      "        2.2548e-05, 1.7510e-05, 2.6002e-06, 3.2933e-05, 9.0954e-05, 6.0803e-06,\n",
      "        7.4240e-06, 4.7514e-05, 1.9514e-05, 1.1230e-05, 1.0845e-05, 1.6397e-05,\n",
      "        4.8130e-05, 3.9446e-05, 1.8240e-05, 2.2534e-05, 1.3658e-05, 4.8399e-05,\n",
      "        4.3906e-05, 2.4272e-05, 2.7143e-05, 2.5978e-04, 3.0792e-05, 2.5967e-05,\n",
      "        2.3914e-05, 1.1197e-05, 3.8580e-05, 3.1907e-05, 6.3401e-06, 4.3095e-05,\n",
      "        1.7597e-05, 4.0744e-06, 7.5574e-06, 4.7081e-06, 5.1869e-06, 9.2582e-06,\n",
      "        5.4702e-05, 2.0060e-05, 5.4650e-05, 1.5807e-06, 1.9454e-05, 3.4182e-05,\n",
      "        3.3293e-05, 4.3644e-05, 1.8411e-05, 8.2278e-06, 8.1118e-06, 2.6692e-05,\n",
      "        1.3831e-05, 7.8942e-05, 7.9220e-05, 7.3443e-06, 1.4494e-05, 1.4363e-06,\n",
      "        5.9718e-06, 7.6622e-06, 4.1042e-06, 7.8329e-06, 3.0463e-06, 7.6978e-06,\n",
      "        6.0204e-06, 1.6251e-05, 3.7593e-06, 2.3579e-05, 2.8899e-05, 7.1139e-05,\n",
      "        8.2456e-05, 1.6256e-04, 8.8816e-05, 7.1527e-05, 9.7105e-06, 8.4376e-05,\n",
      "        3.4071e-06, 6.9901e-06, 2.9603e-06, 7.2994e-06, 1.2658e-05, 4.9821e-06,\n",
      "        1.1035e-06, 6.9371e-06, 6.5977e-06, 2.2978e-05, 1.6119e-05, 5.1136e-05,\n",
      "        4.5492e-06, 8.6163e-06, 6.7114e-06, 1.1376e-05, 1.4780e-06, 1.6390e-05,\n",
      "        2.0631e-05, 2.0803e-06, 1.1164e-05, 3.9095e-06, 1.0523e-05, 7.0669e-06,\n",
      "        7.7422e-06, 4.3258e-04, 2.5250e-04, 1.0641e-05, 5.5711e-06, 2.9983e-06,\n",
      "        2.3853e-06, 5.4067e-06, 3.3233e-06, 2.1177e-06, 4.7935e-06, 3.6973e-06,\n",
      "        6.3006e-06, 4.9350e-06, 6.9571e-06, 4.7750e-05, 5.9201e-05, 6.7999e-06,\n",
      "        1.1242e-05, 5.7648e-06, 2.9368e-05, 4.3632e-05, 7.0573e-06, 5.4694e-06,\n",
      "        9.6347e-06, 1.7862e-05, 6.4210e-05, 9.8505e-07, 1.7999e-05, 5.3382e-06,\n",
      "        1.3519e-06, 2.0445e-06, 3.7065e-06, 1.6092e-06, 2.9033e-06, 1.1697e-06,\n",
      "        1.2536e-06, 7.6676e-07, 3.1050e-06, 5.2521e-06, 2.3965e-06, 1.7509e-06,\n",
      "        2.6507e-06, 9.6577e-06, 6.7421e-05, 7.3869e-06, 1.6012e-04, 1.0070e-04,\n",
      "        2.2316e-05, 3.2702e-05, 2.3743e-05, 4.5841e-05, 1.1346e-05, 2.5666e-05,\n",
      "        9.1668e-06, 1.9171e-04, 5.6987e-06, 1.1216e-05, 1.5007e-05, 1.1507e-05,\n",
      "        2.9545e-06, 7.5782e-05, 1.9225e-05, 5.1923e-05, 2.3844e-05, 1.2324e-04,\n",
      "        1.3492e-04, 4.4614e-06, 8.6047e-06, 4.2231e-06, 1.0686e-05, 2.2144e-06,\n",
      "        1.3232e-06, 2.5781e-06, 1.3005e-06, 3.7879e-06, 1.3065e-05, 2.8281e-05,\n",
      "        4.3420e-06, 1.8265e-05, 2.0660e-05, 6.7880e-06, 3.6759e-05, 5.3891e-05,\n",
      "        6.2182e-06, 2.3040e-05, 1.4244e-04, 3.2538e-03, 8.5481e-02, 1.2351e-03,\n",
      "        1.4401e-04, 9.8507e-06, 2.6559e-05, 8.6829e-05, 2.9268e-04, 4.2775e-05,\n",
      "        7.9453e-06, 1.1987e-04, 6.8987e-07, 8.2203e-05, 5.0227e-05, 2.4718e-04,\n",
      "        1.6075e-05, 5.5459e-05, 9.3997e-05, 2.3694e-04, 7.3751e-06, 1.0123e-02,\n",
      "        4.4749e-04, 1.1939e-05, 2.9263e-05, 6.6911e-05, 4.2823e-04, 7.6800e-06,\n",
      "        1.3933e-04, 1.0261e-04, 5.5863e-05, 1.6806e-04, 1.0077e-04, 4.1708e-04,\n",
      "        2.1011e-03, 4.2733e-03, 3.8039e-04, 6.8077e-03, 2.0282e-05, 2.3945e-05,\n",
      "        4.1622e-04, 7.0590e-03, 1.0124e-04, 1.8629e-04, 4.3408e-05, 2.5238e-04,\n",
      "        3.4051e-06, 7.3973e-04, 3.6242e-05, 6.0765e-04, 4.8842e-05, 1.1910e-05,\n",
      "        5.9687e-05, 7.4391e-03, 3.7154e-03, 1.9911e-04, 1.9138e-05, 8.2743e-04,\n",
      "        1.2430e-04, 1.0851e-01, 1.1234e-04, 1.8993e-03, 1.6085e-05, 1.2411e-03,\n",
      "        1.7171e-04, 1.3934e-03, 4.1922e-04, 8.6356e-04, 5.6361e-05, 5.7117e-06,\n",
      "        1.2391e-05, 2.1837e-04, 1.2685e-02, 4.0449e-05, 4.2414e-06, 1.1841e-04,\n",
      "        9.6919e-05, 7.5852e-04, 2.0384e-05, 1.2268e-05, 3.1115e-04, 2.6331e-05,\n",
      "        2.4828e-04, 1.5365e-04, 1.9018e-05, 1.2253e-05, 7.5728e-06, 1.1695e-04,\n",
      "        1.8912e-04, 1.5500e-03, 8.0443e-04, 4.9157e-05, 2.3623e-03, 6.4467e-05,\n",
      "        6.2696e-05, 2.7282e-05, 2.3461e-05, 1.8185e-04, 1.4086e-04, 3.9745e-06,\n",
      "        4.4469e-05, 1.1483e-03, 3.3291e-05, 2.2145e-03, 1.0467e-04, 3.9292e-04,\n",
      "        2.5668e-03, 2.4082e-04, 2.0469e-05, 2.9276e-05, 4.7883e-05, 3.8829e-05,\n",
      "        1.6833e-05, 2.2733e-05, 7.0416e-05, 4.1098e-04, 7.4674e-05, 2.6345e-03,\n",
      "        2.2782e-04, 1.0362e-05, 4.4632e-03, 3.0816e-05, 1.3048e-04, 9.1950e-05,\n",
      "        1.2928e-04, 3.7507e-04, 6.8706e-04, 9.8614e-06, 7.0603e-05, 7.5162e-05,\n",
      "        2.0599e-04, 1.4290e-04, 7.6833e-05, 1.5962e-04, 3.3270e-04, 5.5349e-05,\n",
      "        6.0099e-05, 4.0614e-05, 3.5234e-06, 1.3229e-06, 2.7387e-05, 5.8816e-05,\n",
      "        1.8427e-05, 3.8266e-04, 2.8722e-03, 2.3283e-04, 9.7871e-06, 2.3838e-04,\n",
      "        1.6968e-05, 3.8258e-06, 2.7946e-05, 1.1983e-04, 1.3888e-04, 9.3230e-03,\n",
      "        1.3553e-02, 5.5945e-06, 4.9362e-05, 3.1881e-06, 9.2557e-05, 2.2504e-05,\n",
      "        1.1395e-03, 5.8849e-05, 6.7322e-05, 3.7538e-06, 2.0744e-04, 2.5384e-05,\n",
      "        9.0629e-05, 7.9956e-06, 2.0389e-04, 4.3959e-04, 4.5129e-03, 2.4558e-06,\n",
      "        5.9761e-04, 2.3166e-06, 1.6863e-03, 3.5442e-05, 4.3691e-05, 8.0284e-05,\n",
      "        7.5918e-05, 1.8024e-04, 6.4721e-04, 9.5153e-04, 2.2017e-05, 1.2825e-05,\n",
      "        2.3571e-06, 2.8924e-04, 2.9759e-03, 7.4583e-03, 4.8149e-05, 3.9760e-04,\n",
      "        2.1746e-04, 2.9754e-03, 1.8197e-05, 2.2966e-04, 1.7267e-05, 1.5700e-04,\n",
      "        3.6904e-04, 1.2228e-06, 8.8980e-05, 1.3489e-05, 2.5245e-05, 2.8610e-05,\n",
      "        4.8253e-05, 1.1015e-02, 5.5851e-05, 1.2161e-05, 4.5371e-03, 1.4693e-04,\n",
      "        5.1604e-04, 2.5617e-04, 1.5268e-04, 1.5664e-06, 5.5966e-04, 4.6915e-04,\n",
      "        3.2732e-05, 7.6903e-05, 3.2755e-04, 1.6165e-03, 5.7440e-05, 1.2746e-02,\n",
      "        3.3296e-04, 1.2879e-03, 2.3487e-04, 2.2868e-05, 2.0163e-04, 3.8130e-05,\n",
      "        2.6624e-05, 2.9868e-06, 5.3396e-04, 5.3540e-04, 8.1085e-06, 2.3368e-03,\n",
      "        7.0428e-05, 1.5108e-03, 3.7138e-05, 1.3020e-03, 7.5159e-06, 1.1669e-04,\n",
      "        7.2356e-06, 2.9914e-05, 1.4450e-04, 3.1573e-04, 1.2937e-05, 1.5457e-03,\n",
      "        1.5882e-04, 6.7876e-03, 2.2838e-04, 4.7986e-05, 7.6583e-05, 4.3718e-04,\n",
      "        4.4423e-04, 1.0010e-04, 8.5600e-04, 5.3393e-05, 8.9967e-02, 9.6081e-05,\n",
      "        5.3447e-05, 3.7982e-04, 2.8766e-05, 2.4306e-05, 3.9297e-05, 7.5193e-04,\n",
      "        1.7514e-05, 1.7757e-05, 1.1998e-04, 1.8555e-05, 8.1916e-05, 1.2899e-05,\n",
      "        1.9214e-04, 1.3046e-01, 5.0133e-05, 4.0824e-03, 4.1291e-05, 4.3418e-06,\n",
      "        5.1201e-05, 2.2253e-04, 6.5751e-05, 9.4371e-06, 2.2900e-05, 6.7645e-05,\n",
      "        1.1910e-01, 1.0348e-03, 3.5253e-04, 1.0912e-04, 7.9090e-05, 3.6808e-03,\n",
      "        2.7866e-04, 6.1483e-06, 2.3269e-04, 5.5548e-05, 5.3248e-05, 1.6932e-03,\n",
      "        8.4742e-07, 3.5537e-04, 9.1170e-05, 4.1149e-05, 8.9573e-06, 8.3512e-05,\n",
      "        5.7113e-04, 2.3423e-03, 2.7099e-05, 2.5289e-03, 4.9293e-04, 5.7645e-04,\n",
      "        1.7826e-04, 7.5840e-06, 2.8806e-05, 5.1250e-06, 6.5978e-06, 1.9190e-05,\n",
      "        3.8238e-04, 3.1830e-05, 7.4496e-05, 1.3199e-03, 6.6011e-04, 1.0462e-04,\n",
      "        8.2716e-04, 2.6240e-02, 1.1025e-04, 7.4207e-07, 2.6458e-05, 2.3699e-04,\n",
      "        7.6362e-04, 6.8866e-05, 9.3288e-04, 2.7519e-04, 4.3750e-05, 3.1811e-04,\n",
      "        4.3405e-05, 2.6772e-05, 4.1881e-04, 4.2134e-05, 1.0067e-05, 4.5349e-03,\n",
      "        4.5223e-05, 8.8072e-05, 6.5558e-06, 4.0723e-04, 2.7308e-04, 5.6230e-05,\n",
      "        1.0906e-04, 6.1557e-04, 3.4847e-04, 1.1782e-04, 4.6147e-05, 1.0442e-03,\n",
      "        4.0656e-05, 1.5661e-04, 3.9536e-04, 2.2185e-04, 3.2631e-05, 1.6823e-04,\n",
      "        2.3724e-04, 5.0702e-05, 8.3288e-05, 4.9480e-05, 1.8945e-05, 4.0948e-05,\n",
      "        2.0095e-05, 3.3310e-06, 1.1945e-04, 4.7525e-05, 8.3112e-05, 8.2488e-04,\n",
      "        2.6992e-05, 3.9611e-04, 1.4424e-04, 3.6740e-05, 1.1101e-05, 3.5903e-05,\n",
      "        4.6437e-05, 6.2254e-05, 6.2469e-05, 2.5314e-05, 1.9556e-04, 3.5327e-04,\n",
      "        3.3986e-04, 9.8292e-04, 2.6674e-04, 2.1466e-05, 1.2145e-04, 7.3454e-06,\n",
      "        1.0846e-04, 9.2593e-06, 2.3186e-04, 7.6614e-05, 9.6410e-05, 2.3655e-03,\n",
      "        1.1889e-04, 2.3883e-04, 5.1622e-05, 1.1553e-04, 7.1244e-06, 6.7204e-06,\n",
      "        3.6590e-05, 1.0585e-02, 2.9753e-03, 2.7224e-05, 4.5930e-04, 6.0003e-04,\n",
      "        4.4416e-05, 2.3272e-05, 4.0557e-05, 1.3948e-04, 1.1710e-05, 1.3453e-05,\n",
      "        3.7810e-04, 3.9656e-04, 8.1745e-05, 1.6671e-05, 7.3352e-03, 1.8233e-04,\n",
      "        2.2100e-05, 8.8534e-05, 4.4342e-05, 7.2586e-04, 1.5729e-05, 1.0079e-05,\n",
      "        3.1577e-05, 8.0023e-06, 4.6630e-04, 6.9205e-05, 2.2931e-06, 5.5412e-06,\n",
      "        2.8429e-04, 1.2773e-03, 8.0081e-04, 8.0603e-06, 7.2696e-05, 2.3740e-05,\n",
      "        6.3834e-04, 3.5310e-06, 1.3928e-04, 8.1620e-05, 4.4754e-05, 2.6685e-06,\n",
      "        1.1347e-02, 2.2982e-05, 8.0350e-03, 1.6159e-02, 2.0461e-03, 2.1177e-05,\n",
      "        4.0728e-04, 3.1463e-04, 3.1483e-04, 5.0738e-05, 5.0073e-05, 4.3640e-04,\n",
      "        1.7355e-04, 1.8450e-05, 2.6726e-05, 1.0858e-04, 3.5530e-04, 1.1434e-03,\n",
      "        8.4298e-05, 1.1313e-06, 6.2825e-04, 7.7776e-04, 5.3209e-07, 1.1576e-04,\n",
      "        2.6831e-05, 6.4665e-04, 2.2591e-04, 2.6154e-03, 1.0278e-03, 9.4435e-05,\n",
      "        1.8883e-06, 9.4810e-05, 7.4228e-05, 2.1210e-05, 4.8831e-04, 1.7362e-03,\n",
      "        2.6444e-05, 5.1338e-06, 1.2796e-04, 9.7008e-06, 8.9555e-07, 4.1043e-04,\n",
      "        5.4264e-03, 4.2484e-05, 3.0467e-05, 6.1794e-04, 1.8151e-05, 4.3141e-05,\n",
      "        6.4328e-05, 1.2342e-03, 8.3969e-05, 2.1765e-04, 3.2420e-05, 6.5143e-03,\n",
      "        1.6089e-05, 1.9917e-03, 1.6624e-05, 8.5387e-05, 7.3476e-04, 7.7779e-06,\n",
      "        2.9820e-04, 3.2451e-05, 5.3939e-04, 8.7074e-05, 3.2976e-04, 5.0920e-04,\n",
      "        4.5675e-05, 5.0585e-04, 1.9039e-03, 2.9776e-02, 1.1808e-04, 3.3625e-04,\n",
      "        2.1187e-02, 3.5139e-04, 1.5540e-05, 6.9094e-05, 2.2861e-04, 1.4703e-03,\n",
      "        3.5661e-06, 1.3089e-05, 5.5776e-06, 5.1896e-06, 7.9430e-05, 1.3751e-04,\n",
      "        1.2981e-04, 8.4594e-06, 2.1860e-05, 1.3715e-03, 4.5702e-05, 5.2738e-04,\n",
      "        2.3506e-05, 1.4047e-04, 3.7998e-05, 3.2833e-04, 5.5601e-05, 3.8029e-04,\n",
      "        1.4568e-05, 1.7516e-05, 6.7520e-05, 4.7510e-06, 4.1208e-05, 2.2657e-05,\n",
      "        1.9282e-04, 8.3163e-05, 4.9686e-05, 1.3668e-05, 8.6237e-05, 1.8050e-05,\n",
      "        4.4180e-05, 4.4795e-05, 2.4130e-05, 6.8166e-06, 4.8794e-06, 3.8601e-05,\n",
      "        7.8021e-05, 4.3396e-05, 8.4491e-06, 1.4756e-05, 1.4160e-05, 3.1457e-05,\n",
      "        1.8903e-05, 2.0227e-05, 5.4694e-05, 1.2973e-05, 2.4955e-04, 6.1231e-05,\n",
      "        4.5619e-05, 2.1415e-04, 1.1259e-05, 1.3935e-05, 5.3846e-05, 1.9568e-05,\n",
      "        1.6192e-03, 4.9590e-05, 4.8071e-04, 5.3587e-04, 1.3684e-05, 1.0806e-03,\n",
      "        5.8567e-06, 6.1800e-06, 3.7547e-05, 5.6575e-06, 1.0256e-05, 1.0953e-04,\n",
      "        1.6849e-05, 9.0902e-06, 8.4928e-06, 5.0502e-04, 9.0661e-03, 3.0953e-05,\n",
      "        9.1781e-05, 4.3624e-05, 1.2349e-06, 6.4299e-05, 2.6817e-05, 1.4074e-05,\n",
      "        2.5225e-06, 1.8287e-06, 5.5790e-06, 1.6212e-06, 3.3107e-06, 1.9645e-05,\n",
      "        1.8461e-05, 5.1269e-06, 1.1054e-04, 1.8999e-03])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('anaconda3-2021.05': pyenv)"
  },
  "interpreter": {
   "hash": "6e1566eebc1c1be0cd319d6a0d9b35248b6256674f9d8a85f0e918d1ebc9692f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}